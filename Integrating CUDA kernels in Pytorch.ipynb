{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6df3914-b613-4772-8542-570e2153e587",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Lecture 1 from CUDA MODE - cuda profilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ac555c-50bc-40a5-bcb9-921b82f8524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bf7c0d-d43e-41a6-96a8-759a650f5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.,2.,3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d534-57a3-4dae-87e5-5dceb4d90d39",
   "metadata": {},
   "source": [
    "### Time a pytorch function on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe9448-3375-457b-936d-8fc6e026f3f4",
   "metadata": {},
   "source": [
    "It is not possible to use the `time` python module because CUDA is Async. \\\n",
    "We can write a function that will use `torch.cuda.Event` to appropriately measure time. There is a warmup time, so the first iteration won't be as fast as the later ones. If we want a measure of the steady state, we have to function we want to measure a few time befaor staring the measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b9e72c-5740-4bb1-82de-88302e2b1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    #warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f33360d7-b42f-4d49-b57f-5cf430d517af",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(10000, 10000).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f556b5a-c4b1-4dac-b896-e16ce1f18b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_2(a):\n",
    "    return a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e578b31-f131-470c-b777-89025ad8508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_3(a):\n",
    "    return a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49443ad1-29ff-4045-aa5b-bf8d2e0b1e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462720155715942"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051cf77-ac14-4a1b-bd75-1e08e3fb46f7",
   "metadata": {},
   "source": [
    "### We can use `torch.autograd.profiler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b22118aa-2b81-4fa6-8a84-41dc94f85d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "467f5a07-07af-478f-82c1-444766499400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-20 13:33:29 513:513 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-20 13:33:29 513:513 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-20 13:33:29 513:513 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    torch.square(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f248afa2-992b-4469-89b7-e53a2b0d2af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "             aten::square         0.81%      11.000us         6.28%      85.000us      85.000us       4.000us         0.44%     913.000us     913.000us             1  \n",
      "                aten::pow         4.06%      55.000us         5.39%      73.000us      73.000us     907.000us        99.34%     909.000us     909.000us             1  \n",
      "        aten::result_type         0.07%       1.000us         0.07%       1.000us       1.000us       1.000us         0.11%       1.000us       1.000us             1  \n",
      "                 aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.11%       1.000us       1.000us             1  \n",
      "          cudaEventRecord         0.81%      11.000us         0.81%      11.000us       1.375us       0.000us         0.00%       0.000us       0.000us             8  \n",
      "         cudaLaunchKernel         1.11%      15.000us         1.11%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        93.13%       1.261ms        93.13%       1.261ms       1.261ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.354ms\n",
      "Self CUDA time total: 913.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff701cd2-9981-437a-af85-2d28307f0f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling a * a\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fddb8b74-2bde-4903-a808-0ceffba17f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-20 13:33:29 513:513 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-20 13:33:29 513:513 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-20 13:33:29 513:513 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ce636ec-0c99-4dab-a8c7-a17e2c2b8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling a * a\n",
      "=============\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::mul         4.41%      54.000us         6.21%      76.000us      76.000us     846.000us       100.00%     846.000us     846.000us             1  \n",
      "          cudaEventRecord         1.06%      13.000us         1.06%      13.000us       6.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "         cudaLaunchKernel         1.80%      22.000us         1.80%      22.000us      22.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        92.73%       1.135ms        92.73%       1.135ms       1.135ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.224ms\n",
      "Self CUDA time total: 846.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "171d1d0c-6561-4e85-9a72-2084d16a9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af506e6e-b2e6-4467-83b8-2551009e168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-20 13:33:30 513:513 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-20 13:33:30 513:513 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-20 13:33:30 513:513 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_3(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43208b9b-bc28-4678-afe0-7ae2d49cacfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::pow         6.49%      83.000us         9.00%     115.000us     115.000us     839.000us        99.76%     841.000us     841.000us             1  \n",
      "        aten::result_type         0.16%       2.000us         0.16%       2.000us       2.000us       1.000us         0.12%       1.000us       1.000us             1  \n",
      "                 aten::to         0.08%       1.000us         0.08%       1.000us       1.000us       1.000us         0.12%       1.000us       1.000us             1  \n",
      "          cudaEventRecord         1.41%      18.000us         1.41%      18.000us       3.000us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "         cudaLaunchKernel         1.88%      24.000us         1.88%      24.000us      24.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize        89.98%       1.150ms        89.98%       1.150ms       1.150ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.278ms\n",
      "Self CUDA time total: 841.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90769966-8dfd-4290-a829-12c3e4a18b20",
   "metadata": {},
   "source": [
    "Running the same code in a python script yields shorter execution times. The iPython environment and the iterative cell mechanics might influence the cuda Events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b79acd-4019-4355-9deb-30bdec72baa5",
   "metadata": {},
   "source": [
    "### We can use the Pytorch profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6dce1ea8-bff5-4b3c-98d2-a421a40630b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009972e6-5b44-44b0-b487-3bb71df528c2",
   "metadata": {},
   "source": [
    "We can save the report in JSON format that can be read by Chrome or Brave tracing, drag and drop in the browser at `chrome://tracing/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc44ac79-e578-4fb5-a279-2ae731c8d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"./log/test_trace_\" + str(prof.step_num) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c1d4903-3db3-45d0-b2a9-cbda6f1303a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-20 18:20:09 513:513 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-20 18:20:10 513:513 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-20 18:20:10 513:513 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            ProfilerStep*         0.11%     999.000us        93.85%     841.049ms     420.524ms             2  \n",
      "              aten::randn         0.00%      24.000us        52.64%     471.796ms     235.898ms             2  \n",
      "              aten::empty         0.00%      19.000us         0.00%      19.000us       9.500us             2  \n",
      "            aten::normal_        52.64%     471.753ms        52.64%     471.753ms     235.876ms             2  \n",
      "                 aten::to         0.00%      26.000us        41.08%     368.126ms      92.031ms             4  \n",
      "           aten::_to_copy         0.00%      23.000us        41.07%     368.100ms     184.050ms             2  \n",
      "      aten::empty_strided         0.00%      25.000us         0.00%      25.000us      12.500us             2  \n",
      "              aten::copy_         0.00%      43.000us        41.07%     368.052ms     184.026ms             2  \n",
      "          cudaMemcpyAsync        40.95%     366.971ms        40.95%     366.971ms     183.486ms             2  \n",
      "    cudaStreamSynchronize         0.12%       1.038ms         0.12%       1.038ms     519.000us             2  \n",
      "             aten::square         0.00%       7.000us         0.01%     128.000us      64.000us             2  \n",
      "                aten::pow         0.01%      81.000us         0.01%     121.000us      60.500us             2  \n",
      "        aten::result_type         0.00%       1.000us         0.00%       1.000us       0.500us             2  \n",
      "         cudaLaunchKernel         0.00%      39.000us         0.00%      39.000us      19.500us             2  \n",
      "    cudaDeviceSynchronize         6.15%      55.145ms         6.15%      55.145ms      55.145ms             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 896.194ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "     schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),\n",
    "    on_trace_ready=trace_handler\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            torch.square(torch.randn(10000, 10000).cuda())\n",
    "            # send a signal to the profiler that the next iteration has started\n",
    "            p.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1f323-7416-4a4f-8096-7201811df7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
