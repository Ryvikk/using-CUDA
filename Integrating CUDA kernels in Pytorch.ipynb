{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6df3914-b613-4772-8542-570e2153e587",
   "metadata": {},
   "source": [
    "### Lecture 1 from CUDA MODE - cuda profilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ac555c-50bc-40a5-bcb9-921b82f8524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bf7c0d-d43e-41a6-96a8-759a650f5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.,2.,3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8d534-57a3-4dae-87e5-5dceb4d90d39",
   "metadata": {},
   "source": [
    "### Time a pytorch function on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe9448-3375-457b-936d-8fc6e026f3f4",
   "metadata": {},
   "source": [
    "It is not possible to use the `time` python module because CUDA is Async. \\\n",
    "We can write a function that will use `torch.cuda.Event` to appropriately measure time. There is a warmup time, so the first iteration won't be as fast as the later ones. If we want a measure of the steady state, we have to function we want to measure a few time befaor staring the measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b9e72c-5740-4bb1-82de-88302e2b1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    #warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33360d7-b42f-4d49-b57f-5cf430d517af",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(10000, 10000).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f556b5a-c4b1-4dac-b896-e16ce1f18b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_2(a):\n",
    "    return a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e578b31-f131-470c-b777-89025ad8508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_3(a):\n",
    "    return a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49443ad1-29ff-4045-aa5b-bf8d2e0b1e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9185600280761719"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051cf77-ac14-4a1b-bd75-1e08e3fb46f7",
   "metadata": {},
   "source": [
    "### We can use `torch.autograd.profiler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22118aa-2b81-4fa6-8a84-41dc94f85d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467f5a07-07af-478f-82c1-444766499400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-19 22:46:45 872:872 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-19 22:46:45 872:872 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-19 22:46:45 872:872 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    torch.square(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f248afa2-992b-4469-89b7-e53a2b0d2af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         aten::square        15.46%      15.000us       100.00%      97.000us      97.000us      15.000us         0.03%      51.006ms      51.006ms             1  \n",
      "            aten::pow        81.44%      79.000us        84.54%      82.000us      82.000us      50.987ms        99.96%      50.991ms      50.991ms             1  \n",
      "    aten::result_type         2.06%       2.000us         2.06%       2.000us       2.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "             aten::to         1.03%       1.000us         1.03%       1.000us       1.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 97.000us\n",
      "Self CUDA time total: 51.006ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddb8b74-2bde-4903-a808-0ceffba17f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2024-02-19 22:44:56 872:872 init.cpp:155] function cbapi->getCuptiStatus() failed with error CUPTI_ERROR_NOT_INITIALIZED (15)\n",
      "WARNING:2024-02-19 22:44:56 872:872 init.cpp:156] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2024-02-19 22:44:56 872:872 init.cpp:158] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n",
      "STAGE:2024-02-19 22:44:56 872:872 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-19 22:44:56 872:872 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-19 22:44:56 872:872 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce636ec-0c99-4dab-a8c7-a17e2c2b8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::mul       100.00%      43.000us       100.00%      43.000us      43.000us      66.190ms       100.00%      66.190ms      66.190ms             1  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 43.000us\n",
      "Self CUDA time total: 66.190ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af506e6e-b2e6-4467-83b8-2551009e168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-19 22:46:02 872:872 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-02-19 22:46:02 872:872 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-19 22:46:02 872:872 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_3(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43208b9b-bc28-4678-afe0-7ae2d49cacfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            aten::pow        99.97%       9.144ms       100.00%       9.147ms       9.147ms      74.016ms        99.99%      74.021ms      74.021ms             1  \n",
      "    aten::result_type         0.02%       2.000us         0.02%       2.000us       2.000us       3.000us         0.00%       3.000us       3.000us             1  \n",
      "             aten::to         0.01%       1.000us         0.01%       1.000us       1.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 9.147ms\n",
      "Self CUDA time total: 74.021ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b79acd-4019-4355-9deb-30bdec72baa5",
   "metadata": {},
   "source": [
    "### We can use the Pytorch profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dce1ea8-bff5-4b3c-98d2-a421a40630b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc44ac79-e578-4fb5-a279-2ae731c8d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_handler(prof):\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c1d4903-3db3-45d0-b2a9-cbda6f1303a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-19 23:30:16 872:872 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "          ProfilerStep*         1.20%      10.021ms       100.00%     832.520ms     416.260ms             2  \n",
      "            aten::randn         0.04%     339.000us        56.69%     471.955ms     235.977ms             2  \n",
      "            aten::empty         0.00%      25.000us         0.00%      25.000us      12.500us             2  \n",
      "          aten::normal_        56.65%     471.591ms        56.65%     471.591ms     235.796ms             2  \n",
      "               aten::to         1.10%       9.147ms        42.09%     350.420ms      87.605ms             4  \n",
      "         aten::_to_copy         0.00%      29.000us        40.99%     341.273ms     170.637ms             2  \n",
      "    aten::empty_strided         0.00%      26.000us         0.00%      26.000us      13.000us             2  \n",
      "            aten::copy_        40.99%     341.218ms        40.99%     341.218ms     170.609ms             2  \n",
      "           aten::square         0.01%      44.000us         0.01%      84.000us      42.000us             2  \n",
      "              aten::pow         0.01%      79.000us         0.01%      80.000us      40.000us             2  \n",
      "      aten::result_type         0.00%       1.000us         0.00%       1.000us       0.500us             2  \n",
      "-----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 832.520ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-02-19 23:30:17 872:872 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-02-19 23:30:17 872:872 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "     schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),\n",
    "    on_trace_ready=trace_handler\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
    "    # used when outputting for tensorboard\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            torch.square(torch.randn(10000, 10000).cuda())\n",
    "            # send a signal to the profiler that the next iteration has started\n",
    "            p.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bae06-4954-48d3-81a2-40ea1b823dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
